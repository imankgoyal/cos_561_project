{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../utils/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import layers\n",
    "\n",
    "class model(object):\n",
    "    \n",
    "    def __init__(self, x, is_training, hid_size, num_hid):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "        \"\"\"\n",
    "        _y = layers.ser_fc_layers(\n",
    "                inp=x,\n",
    "                is_training=is_training,\n",
    "                num_hid=num_hid,\n",
    "                hid_dim=hid_size,\n",
    "                out_dim=1,\n",
    "                relu_after=False,\n",
    "                add_bn=False,\n",
    "                name=\"output\",\n",
    "                reuse=None)\n",
    "        self._y = tf.squeeze(_y, axis=1)\n",
    "        \n",
    "        batchnorm_updates = tf.get_collection(\n",
    "            layers.UPDATE_OPS_COLLECTION)\n",
    "        self.batchnorm_updates_op = tf.group(*batchnorm_updates)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../data/')\n",
    "sys.path.insert(0, '../utils/')\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import data_loader\n",
    "import os\n",
    "\n",
    "\n",
    "class SupervisedModel(object):\n",
    "    \"\"\" Training a supervised model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 loader,\n",
    "                 result_dir,\n",
    "                 hid_size,\n",
    "                 num_hid,\n",
    "                 init_learning_rate=1e-3,\n",
    "                 l2_reg=0.1,\n",
    "                 max_grad_norm=0.5):\n",
    "        \"\"\"\n",
    "        Assumption: access to a direct loader. the batch size directly decided\n",
    "            in the loader\n",
    "        \"\"\"\n",
    "        # initialization\n",
    "        self.loader = loader\n",
    "        self.result_dir = result_dir\n",
    "        self.init_learning_rate = init_learning_rate\n",
    "        self.hid_size = hid_size\n",
    "        self.num_hid = num_hid\n",
    "        self.l2_reg = l2_reg\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        if not os.path.exists(self.result_dir):\n",
    "            os.mkdir(self.result_dir)\n",
    "        if not os.path.exists(self.result_dir.format('/train')):\n",
    "            os.mkdir(self.result_dir.format('/train'))\n",
    "        if not os.path.exists(self.result_dir.format('/validation')):\n",
    "            os.mkdir(self.result_dir.format('/validation'))\n",
    "        if not os.path.exists(self.result_dir.format('/test')):\n",
    "            os.mkdir(self.result_dir.format('/test'))\n",
    "        if not os.path.exists(self.result_dir.format('/model')):\n",
    "            os.mkdir(self.result_dir.format('/model'))\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            # session\n",
    "            self.sess = tf.Session(graph=self.graph)\n",
    "\n",
    "            # placeholders\n",
    "            self.x = tf.placeholder(\n",
    "                tf.float32, shape=[None, self.loader.bin_size])\n",
    "            self.is_training = tf.placeholder(tf.bool, shape=[])\n",
    "            self.y = tf.placeholder(tf.float32, shape=[None])\n",
    "            self.learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "            # model\n",
    "            self.model = model(\n",
    "                x=self.x,\n",
    "                is_training=self.is_training,\n",
    "                hid_size=self.hid_size,\n",
    "                num_hid=self.num_hid)\n",
    "\n",
    "            # losses\n",
    "            self.l2_loss = 2 * tf.nn.l2_loss(self.y - self.model._y)\n",
    "            loss_reg = 0\n",
    "            for v in tf.trainable_variables():\n",
    "                if not 'bias' in v.name.lower():\n",
    "                    loss_reg += tf.nn.l2_loss(v)\n",
    "            loss = self.l2_loss + (self.l2_reg * loss_reg)\n",
    "            self.loss = loss\n",
    "            \n",
    "            # setting up the optmimizer\n",
    "            self.params = tf.trainable_variables()\n",
    "            grads = tf.gradients(loss, self.params)\n",
    "            if self.max_grad_norm is not None:\n",
    "                grads, _grad_norm = tf.clip_by_global_norm(\n",
    "                    grads, self.max_grad_norm)\n",
    "            grads = list(zip(grads, self.params))\n",
    "\n",
    "            trainer = tf.train.AdamOptimizer(\n",
    "                learning_rate=self.learning_rate, epsilon=1e-3)\n",
    "    \n",
    "            # applying the batchnorm update ops with training\n",
    "            self._train = tf.group(self.model.batchnorm_updates_op,\n",
    "                                   trainer.apply_gradients(grads))\n",
    "\n",
    "            # tensorboard summaries\n",
    "            tf.summary.scalar('l2_loss', self.l2_loss, collections=['tr'])\n",
    "            tf.summary.scalar('loss_reg', loss_reg, collections=['tr'])\n",
    "            tf.summary.scalar('loss', loss, collections=['tr'])\n",
    "\n",
    "            # placeholders for logging\n",
    "            self.comp_rmse = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "            tf.summary.scalar(\n",
    "                \"comp_rmse\", self.comp_rmse, collections=['tr_2', 'va', 'te'])\n",
    "\n",
    "            self.merged_tr = tf.summary.merge_all('tr')\n",
    "            self.merged_tr_2 = tf.summary.merge_all('tr_2')\n",
    "            self.merged_te = tf.summary.merge_all('te')\n",
    "            self.merged_va = tf.summary.merge_all('va')\n",
    "\n",
    "            # graph initialization\n",
    "            init_op1 = tf.global_variables_initializer()\n",
    "            self.sess.run(init_op1)\n",
    "            init_opt2 = tf.local_variables_initializer()\n",
    "            self.sess.run(init_opt2)\n",
    "\n",
    "            # setting up tensorboard writers\n",
    "            self.tr_writer = tf.summary.FileWriter(result_dir + '/train',\n",
    "                                                   self.sess.graph)\n",
    "            self.va_writer = tf.summary.FileWriter(result_dir + '/validation')\n",
    "            self.te_writer = tf.summary.FileWriter(result_dir + '/test')\n",
    "            self.saver = tf.train.Saver()\n",
    "\n",
    "        self.steps_taken = 0\n",
    "        # a value divisible by 2400\n",
    "        # 2400 is the number of 'te' and 'va' samples\n",
    "        self.log_batch_size = 2400\n",
    "        self.va_loader = data_loader.loader(\n",
    "            shuffle=False,\n",
    "            bin_size=self.loader.bin_size,\n",
    "            batch_size=self.log_batch_size,\n",
    "            split='va',\n",
    "            mu=self.loader.mu,\n",
    "            sigma=self.loader.sigma)\n",
    "        self.te_loader = data_loader.loader(\n",
    "            shuffle=False,\n",
    "            bin_size=self.loader.bin_size,\n",
    "            batch_size=self.log_batch_size,\n",
    "            split='te',\n",
    "            mu=self.loader.mu,\n",
    "            sigma=self.loader.sigma)\n",
    "\n",
    "    def learn(self, log_loss=10, use_decay=False):\n",
    "        self.steps_taken += 1\n",
    "        x, y = self.loader.load()        \n",
    "        feed_dict = {self.x: x, self.y: y, self.is_training: True}\n",
    "\n",
    "        if use_decay:\n",
    "            feed_dict[self.learning_rate] = (\n",
    "                self.init_learning_rate / np.sqrt(self.steps_taken))\n",
    "        else:\n",
    "            feed_dict[self.learning_rate] = (self.init_learning_rate)\n",
    "        \n",
    "        summary, _, _y, l2_loss = self.sess.run((self.merged_tr, self._train, self.model._y, self.l2_loss),\n",
    "                                   feed_dict=feed_dict)\n",
    "#         print(l2_loss)\n",
    "        if self.steps_taken % log_loss == 0:\n",
    "            self.tr_writer.add_summary(summary, self.steps_taken)\n",
    "\n",
    "    def test_and_log(self, split):\n",
    "        \"\"\" \n",
    "            Args:\n",
    "                split (str): either va' or 'te' \n",
    "        \"\"\"\n",
    "\n",
    "        if split == 'va':\n",
    "            writer = self.va_writer\n",
    "            merged = self.merged_va\n",
    "            loader = self.va_loader\n",
    "        elif split == 'te':\n",
    "            writer = self.te_writer\n",
    "            merged = self.merged_te\n",
    "            loader = self.te_loader\n",
    "        \n",
    "        total_loss = 0\n",
    "        for i in range(loader.num_batches):\n",
    "            x, y = loader.load()\n",
    "            total_loss += self.sess.run(\n",
    "                self.l2_loss,\n",
    "                feed_dict={\n",
    "                    self.x: x,\n",
    "                    self.y: y,\n",
    "                    self.is_training: False\n",
    "                })\n",
    "\n",
    "        rmse = np.sqrt(total_loss / (loader.num_batches * self.log_batch_size))\n",
    "#         print(\"{}_rmse :{}\".format(split, rmse))\n",
    "#         summary = self.sess.run((merged), feed_dict={self.comp_rmse: rmse})\n",
    "#         writer.add_summary(summary, self.steps_taken)\n",
    "        return rmse\n",
    "        \n",
    "    def save(self):\n",
    "        self.saver.save(self.sess, \"{}/{}\".format(self.result_dir, 'model'))\n",
    "    \n",
    "    def restore(self):\n",
    "#         print(\"Restoring the best model.\")\n",
    "        self.saver.restore(self.sess, \"{}/{}\".format(self.result_dir, 'model'))\n",
    "\n",
    "    def y_value(self, x_input):\n",
    "        \"\"\" Returns the predicted y for x_input\n",
    "        \"\"\"\n",
    "\n",
    "        feed_dict = {self.x: x_input, self.is_training: False}\n",
    "        return self.sess.run([self._y], feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_loader \n",
    "train_size = 9600\n",
    "batch_size = 9600\n",
    "log_per_epoch = 10\n",
    "early_stopping = 1000\n",
    "\n",
    "def evaluate(bin_size, hid_size, num_hid, run):\n",
    "    loader = data_loader.loader(\n",
    "        shuffle=False,\n",
    "        bin_size=bin_size,\n",
    "        batch_size=batch_size,\n",
    "        split='tr')\n",
    "    network = SupervisedModel(\n",
    "        model=model,\n",
    "        loader=loader,\n",
    "        result_dir='./{}_{}_{}_{}'.format(bin_size, hid_size, num_hid, run),\n",
    "        num_hid=num_hid,\n",
    "        hid_size=hid_size)\n",
    "\n",
    "    batch_per_epoch = train_size // batch_size\n",
    "    best_val_rmse = 100\n",
    "    best_val_epoch = 0\n",
    "    \n",
    "    for epoch_i in range(100000):\n",
    "        \n",
    "        if epoch_i > (best_val_epoch + early_stopping):\n",
    "            break\n",
    "        \n",
    "        if epoch_i % log_per_epoch == 0:\n",
    "            # validation\n",
    "            cur_val_rmse = network.test_and_log('va')\n",
    "            if cur_val_rmse < best_val_rmse:\n",
    "                best_val_rmse = cur_val_rmse\n",
    "                best_val_epoch = epoch_i\n",
    "                network.save()\n",
    "        \n",
    "        for batch_i in range(batch_per_epoch):\n",
    "            network.learn(use_decay=True)\n",
    "    \n",
    "    network.restore()\n",
    "    return (network.test_and_log('va'), network.test_and_log('te'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1 = [{\n",
    "    'bin_size': 20,\n",
    "    'hid_size': hid_size,\n",
    "    'num_hid': 5\n",
    "} for hid_size in [10, 20, 40, 60, 80, 100]]\n",
    "exp2 = [{\n",
    "    'bin_size': bin_size,\n",
    "    'hid_size': 20,\n",
    "    'num_hid': 5\n",
    "} for bin_size in [10, 20, 40, 60, 80, 100]]\n",
    "exp3 = [{\n",
    "    'bin_size': 20,\n",
    "    'hid_size': 20,\n",
    "    'num_hid': num_hid\n",
    "} for num_hid in [1, 2, 5, 8, 10]]\n",
    "for exp in exp1 + exp2 + exp3:\n",
    "    for run in range(5):\n",
    "        print(\"bin_size: {}, hid_size: {}, num_hid: {}, run: {}\".format(\n",
    "            exp['bin_size'], exp['hid_size'], exp['num_hid'], run))\n",
    "        print(evaluate(exp['bin_size'], exp['hid_size'], exp['num_hid'], run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3 = [{\n",
    "    'bin_size': 20,\n",
    "    'hid_size': 20,\n",
    "    'num_hid': num_hid\n",
    "} for num_hid in [ 2, 8, 10]]\n",
    "for exp in exp3:\n",
    "    for run in range(5):\n",
    "        print(\"bin_size: {}, hid_size: {}, num_hid: {}, run: {}\".format(\n",
    "            exp['bin_size'], exp['hid_size'], exp['num_hid'], run))\n",
    "        print(evaluate(exp['bin_size'], exp['hid_size'], exp['num_hid'], run))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (packing)",
   "language": "python",
   "name": "packing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
